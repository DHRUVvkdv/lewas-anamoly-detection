{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Water Quality Analysis System...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_subplots() got unexpected keyword argument(s): ['height']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 329\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Water Quality Analysis System...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m results, fig \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_all_sensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Show results\u001b[39;00m\n\u001b[1;32m    332\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[4], line 238\u001b[0m, in \u001b[0;36manalyze_all_sensors\u001b[0;34m(sensor_files)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Create subplot figure\u001b[39;00m\n\u001b[1;32m    237\u001b[0m n_sensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sensor_files)\n\u001b[0;32m--> 238\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mmake_subplots\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_sensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubplot_titles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msensor_files\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvertical_spacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_sensors\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sensor_files, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-VirginiaTech/Desktop/Coding/Repos/lewas-anamoly-detection/.venv/lib/python3.11/site-packages/plotly/subplots.py:305\u001b[0m, in \u001b[0;36mmake_subplots\u001b[0;34m(rows, cols, shared_xaxes, shared_yaxes, start_cell, print_grid, horizontal_spacing, vertical_spacing, subplot_titles, column_widths, row_heights, specs, insets, column_titles, row_titles, x_title, y_title, figure, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_subplots\u001b[39m(\n\u001b[1;32m      7\u001b[0m     rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m go\u001b[38;5;241m.\u001b[39mFigure:\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Return an instance of plotly.graph_objs.Figure with predefined subplots\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    configured in 'layout'.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    Figure(...)\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_subplots\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared_xaxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared_yaxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhorizontal_spacing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertical_spacing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubplot_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn_widths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_heights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43minsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_titles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-VirginiaTech/Desktop/Coding/Repos/lewas-anamoly-detection/.venv/lib/python3.11/site-packages/plotly/_subplots.py:350\u001b[0m, in \u001b[0;36mmake_subplots\u001b[0;34m(rows, cols, shared_xaxes, shared_yaxes, start_cell, print_grid, horizontal_spacing, vertical_spacing, subplot_titles, column_widths, row_heights, specs, insets, column_titles, row_titles, x_title, y_title, figure, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m column_widths \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_width\u001b[39m\u001b[38;5;124m\"\u001b[39m, column_widths)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_subplots() got unexpected keyword argument(s): \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    352\u001b[0m             \u001b[38;5;28mlist\u001b[39m(kwargs)\n\u001b[1;32m    353\u001b[0m         )\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Validate coerce inputs\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m#  ### rows ###\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rows, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m rows \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: make_subplots() got unexpected keyword argument(s): ['height']"
     ]
    }
   ],
   "source": [
    "# Water Quality Analysis System: Complete Implementation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive data quality assessment.\n",
    "    \"\"\"\n",
    "    print(\"\\nData Quality Assessment:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    # Check value ranges\n",
    "    print(\"\\nValue Ranges:\")\n",
    "    print(f\"Range: {df['value'].min():.2f} to {df['value'].max():.2f}\")\n",
    "    \n",
    "    # Calculate time gaps\n",
    "    time_gaps = df['datetime'].diff()\n",
    "    print(\"\\nTime Gaps:\")\n",
    "    print(f\"Minimum gap: {time_gaps.min()}\")\n",
    "    print(f\"Maximum gap: {time_gaps.max()}\")\n",
    "    print(f\"Mean gap: {time_gaps.mean()}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated('datetime').sum()\n",
    "    print(f\"\\nDuplicate timestamps: {duplicates}\")\n",
    "    \n",
    "    return {\n",
    "        'missing_values': missing,\n",
    "        'time_gaps': time_gaps,\n",
    "        'duplicates': duplicates\n",
    "    }\n",
    "\n",
    "def create_visualizations(df, sensor_name):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of the sensor data.\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert to naive datetime for plotting\n",
    "    df['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)\n",
    "    \n",
    "    # Add time-based features\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Time series plot\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(df['datetime'], df['value'], 'b-', alpha=0.5)\n",
    "    plt.title(f'{sensor_name} Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Distribution plot\n",
    "    plt.subplot(3, 2, 2)\n",
    "    sns.histplot(data=df, x='value', bins=50)\n",
    "    plt.title(f'Distribution of {sensor_name} Values')\n",
    "    plt.xlabel('Value')\n",
    "    \n",
    "    # 3. Box plot by month\n",
    "    plt.subplot(3, 2, 3)\n",
    "    sns.boxplot(data=df, x='month', y='value')\n",
    "    plt.title(f'{sensor_name} Values by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # 4. Daily pattern\n",
    "    plt.subplot(3, 2, 4)\n",
    "    sns.boxplot(data=df, x='hour', y='value')\n",
    "    plt.title(f'{sensor_name} Values by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # 5. Values by year\n",
    "    plt.subplot(3, 2, 5)\n",
    "    sns.boxplot(data=df, x='year', y='value')\n",
    "    plt.title(f'{sensor_name} Values by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # 6. Time gaps analysis\n",
    "    plt.subplot(3, 2, 6)\n",
    "    time_gaps = df['datetime'].diff().dt.total_seconds() / 60  # Convert to minutes\n",
    "    sns.histplot(time_gaps[time_gaps < time_gaps.quantile(0.95)])  # Exclude extreme gaps\n",
    "    plt.title('Distribution of Time Gaps')\n",
    "    plt.xlabel('Gap (minutes)')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "class SensorAnalyzer:\n",
    "    def __init__(self, sensor_name, data):\n",
    "        self.sensor_name = sensor_name\n",
    "        self.data = data\n",
    "        self.results = None\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def preprocess_data(self, resample_freq='1H'):\n",
    "        \"\"\"Preprocess sensor data\"\"\"\n",
    "        df_copy = self.data.copy()\n",
    "        df_copy['datetime'] = pd.to_datetime(df_copy['datetime'])\n",
    "        df_copy.set_index('datetime', inplace=True)\n",
    "        df_resampled = df_copy['value'].resample(resample_freq).mean()\n",
    "        df_resampled = df_resampled.interpolate(method='linear')\n",
    "        return pd.DataFrame(df_resampled.values, index=df_resampled.index, columns=['value'])\n",
    "    \n",
    "    def detect_anomalies(self, seq_length=24):\n",
    "        \"\"\"Run LSTM-based anomaly detection with improved architecture\"\"\"\n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(self.data['value'].values.reshape(-1, 1))\n",
    "        \n",
    "        # Create sequences\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(scaled_data) - seq_length):\n",
    "            sequences.append(scaled_data[i:i + seq_length])\n",
    "            targets.append(scaled_data[i + seq_length])\n",
    "        X = np.array(sequences)\n",
    "        y = np.array(targets)\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:train_size], X[train_size:]\n",
    "        y_train, y_val = y[:train_size], y[train_size:]\n",
    "        \n",
    "        # Build model with improved architecture\n",
    "        model = Sequential([\n",
    "            LSTM(32, activation='relu', input_shape=(seq_length, 1), return_sequences=True),\n",
    "            Dropout(0.3),\n",
    "            LSTM(16, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(8, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Use reduced learning rate and add decay\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "        # Improved callbacks\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            min_delta=0.0001,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_delta=0.0001\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        # Calculate reconstruction error\n",
    "        reconstruction_error = np.mean(np.abs(predictions - y), axis=1)\n",
    "        error_threshold = np.mean(reconstruction_error) + 3 * np.std(reconstruction_error)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results = pd.DataFrame()\n",
    "        results.index = self.data.index[seq_length:]\n",
    "        results['original_value'] = self.data['value'].values[seq_length:]\n",
    "        results['predicted_value'] = scaler.inverse_transform(predictions).flatten()\n",
    "        results['reconstruction_error'] = reconstruction_error\n",
    "        results['is_anomaly'] = reconstruction_error > error_threshold\n",
    "        \n",
    "        # Store results\n",
    "        self.results = results\n",
    "        self.model = model\n",
    "        self.history = history\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_sensor_data(file_path):\n",
    "    \"\"\"Load and basic preprocessing of sensor data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "    return df.sort_values('datetime')\n",
    "\n",
    "# The rest of the code remains the same until the analyze_all_sensors function\n",
    "def analyze_all_sensors(sensor_files):\n",
    "    \"\"\"Analyze all sensor data files\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Create subplot figure - Fixed the subplot creation\n",
    "    n_sensors = len(sensor_files)\n",
    "    fig = make_subplots(\n",
    "        rows=n_sensors,\n",
    "        cols=1,\n",
    "        subplot_titles=[f.stem for f in sensor_files],\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    # Update layout with height - Moved height parameter here\n",
    "    fig.update_layout(\n",
    "        height=300 * n_sensors,\n",
    "        title_text=\"Multi-Sensor Anomaly Detection Results\",\n",
    "        showlegend=True,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    for idx, file_path in enumerate(sensor_files, 1):\n",
    "        print(f\"\\nProcessing {file_path.stem}...\")\n",
    "        \n",
    "        # Load data\n",
    "        data = load_sensor_data(file_path)\n",
    "        \n",
    "        # Perform data quality assessment\n",
    "        quality_metrics = assess_data_quality(data)\n",
    "        \n",
    "        # Create basic visualizations\n",
    "        vis_fig = create_visualizations(data, file_path.stem)\n",
    "        plt.close(vis_fig)  # Close matplotlib figure to free memory\n",
    "        \n",
    "        # Analyze data\n",
    "        analyzer = SensorAnalyzer(file_path.stem, data)\n",
    "        processed_data = analyzer.preprocess_data()\n",
    "        results_df = analyzer.detect_anomalies()\n",
    "        \n",
    "        results[file_path.stem] = {\n",
    "            'analyzer': analyzer,\n",
    "            'results': results_df,\n",
    "            'quality_metrics': quality_metrics\n",
    "        }\n",
    "        \n",
    "        # Add traces to subplot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=results_df.index,\n",
    "                y=results_df['original_value'],\n",
    "                mode='lines',\n",
    "                name=f'{file_path.stem} Original',\n",
    "                line=dict(color='blue', width=1)\n",
    "            ),\n",
    "            row=idx, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=results_df.index,\n",
    "                y=results_df['predicted_value'],\n",
    "                mode='lines',\n",
    "                name=f'{file_path.stem} Predicted',\n",
    "                line=dict(color='green', width=1, dash='dash')\n",
    "            ),\n",
    "            row=idx, col=1\n",
    "        )\n",
    "        \n",
    "        anomalies = results_df[results_df['is_anomaly']]\n",
    "        if not anomalies.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=anomalies.index,\n",
    "                    y=anomalies['original_value'],\n",
    "                    mode='markers',\n",
    "                    name=f'{file_path.stem} Anomalies',\n",
    "                    marker=dict(color='red', size=8, symbol='x')\n",
    "                ),\n",
    "                row=idx, col=1\n",
    "            )\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nSummary for {file_path.stem}:\")\n",
    "        print(f\"Total observations: {len(results_df)}\")\n",
    "        print(f\"Anomalies detected: {results_df['is_anomaly'].sum()}\")\n",
    "        print(f\"Final training loss: {analyzer.history.history['loss'][-1]:.4f}\")\n",
    "        print(f\"Final validation loss: {analyzer.history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    return results, fig\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get all sensor files\n",
    "    sensor_files = list(Path('.').glob('*.csv'))\n",
    "    \n",
    "    # Run analysis\n",
    "    print(\"Starting Water Quality Analysis System...\")\n",
    "    results, fig = analyze_all_sensors(sensor_files)\n",
    "    \n",
    "    # Show results\n",
    "    fig.show()\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(\"\\nOverall Analysis Complete!\")\n",
    "    print(f\"Total sensors analyzed: {len(results)}\")\n",
    "    for sensor_name, data in results.items():\n",
    "        anomaly_count = data['results']['is_anomaly'].sum()\n",
    "        total_obs = len(data['results'])\n",
    "        print(f\"\\n{sensor_name}:\")\n",
    "        print(f\"  - Anomalies: {anomaly_count} ({(anomaly_count/total_obs)*100:.2f}%)\")\n",
    "        print(f\"  - Training Loss: {data['analyzer'].history.history['loss'][-1]:.4f}\")\n",
    "        print(f\"  - Validation Loss: {data['analyzer'].history.history['val_loss'][-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n",
      "\n",
      "Dataset Overview:\n",
      "Time Range: 2015-02-21 21:28:19+00:00 to 2018-03-22 16:39:15+00:00\n",
      "Number of readings: 456444\n",
      "Sampling frequency: 0 days 00:03:32.913016521\n",
      "\n",
      "Basic statistics of DO values:\n",
      "count    456444.000000\n",
      "mean          9.123842\n",
      "std           2.074245\n",
      "min           0.000000\n",
      "25%           8.170000\n",
      "50%           8.920000\n",
      "75%           9.980000\n",
      "max          41.470000\n",
      "Name: value, dtype: float64\n",
      "\n",
      "Data Quality Assessment:\n",
      "\n",
      "Missing Values:\n",
      "sensor_id    452157\n",
      "stderr       456444\n",
      "dtype: int64\n",
      "\n",
      "Value Ranges:\n",
      "DO Range: 0.00 to 41.47 mg/L\n",
      "\n",
      "Time Gaps:\n",
      "Minimum gap: 0 days 00:00:00\n",
      "Maximum gap: 124 days 22:51:25\n",
      "Mean gap: 0 days 00:03:32.913016521\n",
      "\n",
      "Duplicate timestamps: 343\n",
      "\n",
      "Data preprocessing completed successfully!\n",
      "\n",
      "Anomaly Detection Summary:\n",
      "Total observations: 26996\n",
      "Total anomalies detected: 13\n",
      "Sensor faults: 0\n",
      "Environmental anomalies: 13\n",
      "\n",
      "Temporal Distribution of Anomalies:\n",
      "month\n",
      "2     1\n",
      "5     1\n",
      "7     1\n",
      "8     7\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Water Quality Analysis System: LEWAS Lab\n",
    "# Anomaly Detection for Dissolved Oxygen Data\n",
    "\n",
    "## 1. Setup and Configuration\n",
    "# First, we'll import all necessary libraries and set up our environment:\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "\n",
    "## 2. Data Loading and Initial Processing\n",
    "# load and process the water_DO.csv file:\n",
    "\n",
    "def load_do_data(file_path='water_DO.csv'):\n",
    "    \"\"\"\n",
    "    Load and process LEWAS Lab dissolved oxygen data with proper datetime parsing.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "    df = df.sort_values('datetime')\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"Time Range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "    print(f\"Number of readings: {len(df)}\")\n",
    "    print(f\"Sampling frequency: {df['datetime'].diff().mean()}\")\n",
    "    print(\"\\nBasic statistics of DO values:\")\n",
    "    print(df['value'].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "do_data = load_do_data('water_DO.csv')\n",
    "\n",
    "\n",
    "## 3. Data Quality Assessment\n",
    "# Now check the quality of our data:\n",
    "\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive data quality assessment.\n",
    "    \"\"\"\n",
    "    print(\"\\nData Quality Assessment:\")\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    print(\"\\nValue Ranges:\")\n",
    "    print(f\"DO Range: {df['value'].min():.2f} to {df['value'].max():.2f} mg/L\")\n",
    "    \n",
    "    time_gaps = df['datetime'].diff()\n",
    "    print(\"\\nTime Gaps:\")\n",
    "    print(f\"Minimum gap: {time_gaps.min()}\")\n",
    "    print(f\"Maximum gap: {time_gaps.max()}\")\n",
    "    print(f\"Mean gap: {time_gaps.mean()}\")\n",
    "    \n",
    "    duplicates = df.duplicated('datetime').sum()\n",
    "    print(f\"\\nDuplicate timestamps: {duplicates}\")\n",
    "    \n",
    "    return {\n",
    "        'missing_values': missing,\n",
    "        'time_gaps': time_gaps,\n",
    "        'duplicates': duplicates\n",
    "    }\n",
    "\n",
    "# Assess data quality\n",
    "quality_metrics = assess_data_quality(do_data)\n",
    "\n",
    "\n",
    "## 4. Data Visualization\n",
    "# create comprehensive visualizations:\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of the DO data using matplotlib instead of plotly.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['datetime'] = df['datetime'].dt.tz_localize(None)\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(df['datetime'], df['value'], 'b-', alpha=0.5)\n",
    "    plt.title('Dissolved Oxygen Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('DO (mg/L)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(3, 2, 2)\n",
    "    sns.histplot(data=df, x='value', bins=50)\n",
    "    plt.title('Distribution of DO Values')\n",
    "    plt.xlabel('DO (mg/L)')\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    sns.boxplot(data=df, x='month', y='value')\n",
    "    plt.title('DO Values by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('DO (mg/L)')\n",
    "    \n",
    "    plt.subplot(3, 2, 4)\n",
    "    sns.boxplot(data=df, x='hour', y='value')\n",
    "    plt.title('DO Values by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('DO (mg/L)')\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    sns.boxplot(data=df, x='year', y='value')\n",
    "    plt.title('DO Values by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('DO (mg/L)')\n",
    "    \n",
    "    plt.subplot(3, 2, 6)\n",
    "    time_gaps = df['datetime'].diff().dt.total_seconds() / 60\n",
    "    sns.histplot(time_gaps[time_gaps < time_gaps.quantile(0.95)])\n",
    "    plt.title('Distribution of Time Gaps')\n",
    "    plt.xlabel('Gap (minutes)')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('do_analysis.png')  # Save the plot instead of showing it\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations\n",
    "\n",
    "\n",
    "## 5. Data Preprocessing\n",
    "# Now prepare the data for anomaly detection:\n",
    "\n",
    "\n",
    "def preprocess_data(df, resample_freq='1H'):\n",
    "    \"\"\"\n",
    "    Preprocess the DO data for anomaly detection.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy['datetime'] = df_copy['datetime'].dt.tz_localize(None)\n",
    "    df_copy.set_index('datetime', inplace=True)\n",
    "    \n",
    "    df_resampled = df_copy['value'].resample(resample_freq).mean()\n",
    "    df_resampled = df_resampled.interpolate(method='linear')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(df_resampled.values.reshape(-1, 1))\n",
    "    \n",
    "    processed_df = pd.DataFrame(\n",
    "        scaled_values, \n",
    "        index=df_resampled.index, \n",
    "        columns=['scaled_value']\n",
    "    )\n",
    "    processed_df['original_value'] = df_resampled.values\n",
    "    \n",
    "    return processed_df, scaler\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "    # Create visualizations\n",
    "create_visualizations(do_data)\n",
    "    \n",
    "    # Preprocess data for anomaly detection\n",
    "processed_data, scaler = preprocess_data(do_data)\n",
    "    \n",
    "print(\"\\nData preprocessing completed successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "## 6. Anomaly Detection\n",
    "# implement both statistical and LSTM-based anomaly detection:\n",
    "\n",
    "def detect_anomalies(df, window_size=24, z_threshold=3):\n",
    "    \"\"\"\n",
    "    Detect anomalies using statistical methods.\n",
    "    \"\"\"\n",
    "    rolling_mean = df['scaled_value'].rolling(window=window_size, center=True).mean()\n",
    "    rolling_std = df['scaled_value'].rolling(window=window_size, center=True).std()\n",
    "    \n",
    "    z_scores = abs((df['scaled_value'] - rolling_mean) / rolling_std)\n",
    "    \n",
    "    df['is_anomaly'] = z_scores > z_threshold\n",
    "    df['z_score'] = z_scores\n",
    "    \n",
    "    df['anomaly_type'] = 'normal'\n",
    "    df.loc[z_scores > z_threshold * 2, 'anomaly_type'] = 'sensor_fault'\n",
    "    df.loc[(z_scores > z_threshold) & (z_scores <= z_threshold * 2), \n",
    "           'anomaly_type'] = 'environmental'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Detect anomalies\n",
    "data_with_anomalies = detect_anomalies(processed_data)\n",
    "\n",
    "## 7. Visualize Results\n",
    "# create interactive visualizations of the anomalies:\n",
    "\n",
    "def plot_anomalies(df):\n",
    "    \"\"\"\n",
    "    Create visualization of anomaly detection results using matplotlib instead of plotly.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot original data\n",
    "    plt.plot(df.index, df['original_value'], 'b-', label='Original Data', alpha=0.5)\n",
    "    \n",
    "    # Plot sensor faults\n",
    "    sensor_faults = df[df['anomaly_type'] == 'sensor_fault']\n",
    "    plt.scatter(sensor_faults.index, sensor_faults['original_value'], \n",
    "               color='red', marker='x', s=100, label='Sensor Faults')\n",
    "    \n",
    "    # Plot environmental anomalies\n",
    "    env_anomalies = df[df['anomaly_type'] == 'environmental']\n",
    "    plt.scatter(env_anomalies.index, env_anomalies['original_value'],\n",
    "               color='green', marker='o', s=100, label='Environmental Anomalies')\n",
    "    \n",
    "    plt.title(\"Dissolved Oxygen Anomaly Detection Results\")\n",
    "    plt.xlabel(\"DateTime\")\n",
    "    plt.ylabel(\"Dissolved Oxygen (mg/L)\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('anomalies.png')  # Save the plot instead of showing it\n",
    "    plt.close()\n",
    "\n",
    "# Plot results\n",
    "plot_anomalies(data_with_anomalies)\n",
    "\n",
    "\n",
    "## 8. Analysis Summary\n",
    "# create a summary of our findings:\n",
    "\n",
    "\n",
    "def create_summary(df):\n",
    "    \"\"\"\n",
    "    Create a summary of the anomaly detection results.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnomaly Detection Summary:\")\n",
    "    print(f\"Total observations: {len(df)}\")\n",
    "    print(f\"Total anomalies detected: {df['is_anomaly'].sum()}\")\n",
    "    print(f\"Sensor faults: {(df['anomaly_type'] == 'sensor_fault').sum()}\")\n",
    "    print(f\"Environmental anomalies: {(df['anomaly_type'] == 'environmental').sum()}\")\n",
    "    \n",
    "    print(\"\\nTemporal Distribution of Anomalies:\")\n",
    "    df['month'] = df.index.month\n",
    "    monthly_anomalies = df[df['is_anomaly']].groupby('month').size()\n",
    "    print(monthly_anomalies)\n",
    "\n",
    "# Create summary\n",
    "create_summary(data_with_anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
